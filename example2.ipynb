{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeeFaith\\anaconda3\\envs\\kenny\\Lib\\site-packages\\mlflow\\models\\signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  inputs = _infer_schema(model_input) if model_input is not None else None\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # load dataset and train model\n",
    "    df = pd.read_csv('Admission_Predict.csv')\n",
    "    x = df[['gre', 'sop', 'cgpa']]\n",
    "    y = df['admitted']\n",
    "\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "    C = 1\n",
    "    intercept_scaling = 1\n",
    "\n",
    "    def eval_metrics(actual, pred):\n",
    "        acc_score = accuracy_score(actual, pred)\n",
    "        pre_score = precision_score(actual, pred)\n",
    "        f_score = f1_score(actual, pred)\n",
    "\n",
    "        return acc_score, pre_score, f_score\n",
    "\n",
    "    lr = LogisticRegression(C=C, intercept_scaling=intercept_scaling)\n",
    "    prediction = lr.fit(x_train, y_train)\n",
    "\n",
    "    #log metrics\n",
    "    (acc_score, pre_score, f_score) = eval_metrics(y_test, prediction)\n",
    "    print('Logistic Regression (C={:f}, intercept_scaling{:f}:'.format(C, intercept_scaling))\n",
    "    print(\"accuracy score: %s\" % acc_score)\n",
    "    print(\"precision score score: %s\" % pre_score)\n",
    "    print(\"f1 score: %s\" % f_score)\n",
    "\n",
    "    mlflow.log_metric('accuracy score', acc_score)\n",
    "    mlflow.log_metric('precision score', pre_score)\n",
    "    mlflow.log_metric('f1 score', f_score)\n",
    "\n",
    "    # log model params\n",
    "    mlflow.log_param(\"C\", lr.C)\n",
    "    mlflow.log_param(\"Intercept Scaling\", lr.intercept_scaling)\n",
    "    signature = infer_signature(x_train, lr.predict(x_train))\n",
    "\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(lr, \"lr_models\", signature=signature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kenny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
